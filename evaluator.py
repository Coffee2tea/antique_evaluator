import openai
import base64
import requests
from typing import List, Dict, Optional
import re
from config import OPENAI_API_KEY, GPT_MODEL, MAX_TOKENS, TEMPERATURE
import logging
import os
import time
import json

logger = logging.getLogger(__name__)

class AntiqueEvaluator:
    def __init__(self):
        # Get API key from environment variables (loaded from .env file)
        self.api_key = OPENAI_API_KEY
        
        if not self.api_key:
            raise ValueError("OpenAI API key not found. Please set OPENAI_API_KEY in Streamlit secrets (for cloud deployment) or in your .env file/environment variables (for local development).")
        
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # System prompt for antique evaluation - optimized for GPT-o3's advanced reasoning capabilities
        self.system_prompt = self._get_system_prompt()
    
    def evaluate_antique(self, image_urls: list = None, uploaded_files: list = None, descriptions: list = None, title: str = None, language: str = "en") -> dict:
        """
        Evaluate an antique based on images and descriptions
        
        Args:
            image_urls: List of image URLs
            uploaded_files: List of uploaded file data URLs  
            descriptions: List of text descriptions
            title: Title of the antique
            language: Language preference ("zh" for Chinese, "en" for English)
        
        Returns:
            Dict containing evaluation results
        """
        try:
            # Use the language-specific system prompt
            system_prompt = self._get_system_prompt(language)
            
            # Prepare the images for API call
            all_images = []
            if uploaded_files:
                all_images.extend(uploaded_files)
            if image_urls:
                all_images.extend(image_urls)
            
            # Build the user message content with images
            user_message_content = []
            
            # Add text content
            text_message = self._build_user_message(image_urls, uploaded_files, descriptions, title, language)
            user_message_content.append({
                "type": "text",
                "text": text_message
            })
            
            # Add images if available
            if all_images:
                for image in all_images[:6]:  # Limit to 6 images
                    try:
                        if image.startswith('data:image/'):
                            user_message_content.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": image,
                                    "detail": "high"
                                }
                            })
                        else:
                            # Process regular URL
                            base64_image = self._encode_image_from_url(image)
                            if base64_image:
                                user_message_content.append({
                                    "type": "image_url",
                                    "image_url": {
                                        "url": base64_image,
                                        "detail": "high"
                                    }
                                })
                    except Exception as e:
                        logger.warning(f"Failed to process image {image}: {e}")
                        continue
            
            # Make API call with both text and images
            response = self.client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message_content}
                ],
                max_completion_tokens=4000
            )
            
            # Extract the evaluation text
            evaluation_content = response.choices[0].message.content
            
            # Parse the JSON response and extract all data
            parsed_data = self._parse_json_response(evaluation_content)
            
            # Extract score from parsed data (more reliable than direct extraction)
            authenticity_score = parsed_data.get('authenticity_score', 50)
            
            # Use the cleaned detailed_report from parsed data for formatting
            formatted_evaluation = self.format_evaluation_report(parsed_data.get('detailed_report', evaluation_content), language)

            return {
                "success": True,
                "evaluation": formatted_evaluation,
                "score": authenticity_score,
                "raw_content": evaluation_content,
                "parsed_data": parsed_data  # Include parsed data for debugging
            }
            
        except Exception as e:
            logger.error(f"Error in evaluate_antique: {str(e)}")
            error_msg = "é‰´å®šè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•" if language == "zh" else "An error occurred during authentication, please try again later"
            return {
                "success": False,
                "error": error_msg,
                "score": 0
            }
    
    def _prepare_user_prompt(self, descriptions: List[str], title: str) -> str:
        """Prepare the user prompt with context information"""
        prompt_parts = []
        
        if title:
            prompt_parts.append(f"**å¤è‘£æ ‡é¢˜**: {title}")
        
        if descriptions:
            prompt_parts.append("**èƒŒæ™¯ä¿¡æ¯**:")
            for i, desc in enumerate(descriptions, 1):
                if desc.strip():
                    prompt_parts.append(f"{i}. {desc}")
        
        main_request = """
        **ä¸“ä¸šé‰´å®šä»»åŠ¡**
        
        è¯·å¯¹å›¾ç‰‡ä¸­çš„å¤è‘£è¿›è¡Œä¸“ä¸šé‰´å®šåˆ†æã€‚

        **åˆ†æè¦æ±‚ï¼š**
        1. **å…¨é¢è§‚å¯Ÿ**ï¼šä»”ç»†è§‚å¯Ÿå›¾ç‰‡ä¸­å¤è‘£çš„å„ä¸ªè§’åº¦å’Œç»†èŠ‚
        2. **ä¸“ä¸šåˆ¤æ–­**ï¼šè¿ç”¨å¤è‘£é‰´å®šçš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆ†æ
        3. **è¯æ®æ”¯æ’‘**ï¼šåŸºäºå¯è§çš„è§†è§‰è¯æ®å¾—å‡ºç»“è®º
        4. **ç»¼åˆè¯„ä¼°**ï¼šä»å·¥è‰ºã€æè´¨ã€é£æ ¼ã€å†å²èƒŒæ™¯ç­‰ç»´åº¦åˆ†æ
        5. **å‚è€ƒå¯¹æ¯”**ï¼šé€‚å½“å‚è€ƒç”¨æˆ·æä¾›çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä½†ä»¥å›¾åƒåˆ†æä¸ºä¸»
        
        **è¾“å‡ºæ ¼å¼**ï¼š
        è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - authenticity_score: çœŸä¼ªå¯ä¿¡åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰
        - category: å¤è‘£ç±»å‹
        - period: å†å²æ—¶æœŸ
        - material: æè´¨æè¿°
        - brief_analysis: ç®€è¦åˆ†ææ€»ç»“
        - detailed_report: è¯¦ç»†åˆ†ææŠ¥å‘Šï¼ˆå¿…é¡»åŒ…å«å®Œæ•´çš„7ä¸ªéƒ¨åˆ†ï¼šåŸºç¡€ä¿¡æ¯è¯†åˆ«ã€å·¥è‰ºæŠ€æœ¯åˆ†æã€çœŸä¼ªç»¼åˆåˆ¤æ–­ã€ä»·å€¼è¯„ä¼°ã€è¯„åˆ†ç†ç”±åˆ†æ(Pros vs. Cons)ã€æœ€ç»ˆé‰´å®šç»“è®º(Final Authentication Results)ã€ä¸“ä¸šå»ºè®®ä¸ä¿å…»æŒ‡å¯¼ï¼‰
        
        è¯·å¼€å§‹ä¸“ä¸šåˆ†æï¼Œåªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚
        """
        
        prompt_parts.append(main_request)
        
        return "\n\n".join(prompt_parts)
    
    def _prepare_image_content(self, image_urls: List[str]) -> List[Dict]:
        """Prepare image content for the API call - handles both data URLs and regular URLs"""
        image_content = []
        successful_images = 0
        
        for url in image_urls[:6]:  # Limit to 6 images to avoid token limits
            try:
                # Check if it's already a base64 data URL (from file upload)
                if url.startswith('data:image/'):
                    # Debug: Log the first 100 characters of the data URL
                    logger.info(f"Processing data URL: {url[:100]}...")
                    
                    # It's already a base64 data URL, use it directly
                    image_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": url,
                            "detail": "high"
                        }
                    })
                    successful_images += 1
                    logger.info(f"Successfully processed uploaded image {successful_images}")
                else:
                    # It's a regular URL, download and encode it
                    base64_image = self._encode_image_from_url(url)
                    if base64_image:
                        # Debug: Log the first 100 characters of the encoded image
                        logger.info(f"Processing encoded URL: {base64_image[:100]}...")
                        
                        image_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": base64_image,
                                "detail": "high"
                            }
                        })
                        successful_images += 1
                        logger.info(f"Successfully processed image {successful_images}: {url[:50]}...")
                    else:
                        logger.warning(f"Failed to encode image: {url}")
                
            except Exception as e:
                logger.warning(f"Failed to process image {url}: {e}")
                continue
        
        logger.info(f"Successfully processed {successful_images} images out of {len(image_urls)}")
        logger.info(f"Total image_content items: {len(image_content)}")
        
        if successful_images == 0:
            logger.error("No images could be processed")
        
        return image_content
    
    def _encode_image_from_url(self, url: str) -> Optional[str]:
        """Download and encode image to base64"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # Encode to base64
            encoded_image = base64.b64encode(response.content).decode('utf-8')
            
            # Determine the image format
            content_type = response.headers.get('content-type', '')
            if 'jpeg' in content_type or 'jpg' in content_type:
                mime_type = 'image/jpeg'
            elif 'png' in content_type:
                mime_type = 'image/png'
            elif 'webp' in content_type:
                mime_type = 'image/webp'
            else:
                mime_type = 'image/jpeg'  # Default
            
            return f"data:{mime_type};base64,{encoded_image}"
            
        except Exception as e:
            logger.warning(f"Failed to encode image from {url}: {e}")
            return None
    
    def _extract_authenticity_score(self, content: str) -> int:
        """Extract authenticity score from evaluation content"""
        import re
        
        # First, try to find the score in structured sections (more reliable)
        structured_patterns = [
            # Look for scores in the Authentication Assessment section
            r'(?:Authentication Assessment|é‰´å®šè¯„ä¼°).*?(?:Confidence score|å¯ä¿¡åº¦è¯„åˆ†)[ï¼š:\s]*(\d+)%?',
            r'(?:Confidence score|å¯ä¿¡åº¦è¯„åˆ†)[ï¼š:\s]*(\d+)%?',
            # Look for final confidence scores
            r'(?:Final confidence|æœ€ç»ˆå¯ä¿¡åº¦)[ï¼š:\s]*(\d+)%?',
            r'(?:Overall confidence|æ€»ä½“å¯ä¿¡åº¦)[ï¼š:\s]*(\d+)%?',
            # Look for authenticity percentages
            r'(?:Authenticity|çœŸå“å¯èƒ½æ€§)[ï¼š:\s]*(\d+)%?',
        ]
        
        # Try structured patterns first (they are more reliable)
        for pattern in structured_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            if matches:
                try:
                    score = int(matches[-1])  # Take the last match (most likely the final assessment)
                    if 0 <= score <= 100:
                        return score
                except ValueError:
                    continue
        
        # Fallback: Look for any percentage scores, but prioritize those near confidence-related terms
        confidence_context_patterns = [
            # Look for percentages within 50 characters of confidence-related words
            r'(?:confidence|å¯ä¿¡åº¦|authenticity|çœŸå“).{0,50}?(\d+)%',
            r'(\d+)%?.{0,50}?(?:confidence|å¯ä¿¡åº¦|authenticity|çœŸå“)',
        ]
        
        for pattern in confidence_context_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    score = int(matches[-1])  # Take the last match
                    if 0 <= score <= 100:
                        return score
                except ValueError:
                    continue
        
        # Last resort: any percentage in the text
        general_patterns = [
            r'(\d+)%',
        ]
        
        for pattern in general_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                # Filter out unrealistic scores and take the most reasonable one
                valid_scores = []
                for match in matches:
                    try:
                        score = int(match)
                        if 0 <= score <= 100:
                            valid_scores.append(score)
                    except ValueError:
                        continue
                
                if valid_scores:
                    # Prefer scores that are commonly used in authentication (multiples of 5)
                    preferred_scores = [s for s in valid_scores if s % 5 == 0]
                    if preferred_scores:
                        return preferred_scores[-1]  # Take the last one
                    else:
                        return valid_scores[-1]  # Take the last valid score
        
        # Default score based on confidence keywords (unchanged)
        content_lower = content.lower()
        if any(word in content_lower for word in ['é«˜å¯ä¿¡åº¦', 'high confidence', 'å¾ˆå¯èƒ½æ˜¯çœŸå“', 'likely authentic']):
            return 85
        elif any(word in content_lower for word in ['ä¸­ç­‰å¯ä¿¡åº¦', 'moderate confidence', 'éœ€è¦è¿›ä¸€æ­¥', 'further examination']):
            return 70
        elif any(word in content_lower for word in ['è¾ƒä½å¯ä¿¡åº¦', 'low confidence', 'å­˜åœ¨ç–‘ç‚¹', 'concerns present']):
            return 45
        elif any(word in content_lower for word in ['ä½å¯ä¿¡åº¦', 'very low confidence', 'ä»¿åˆ¶å“', 'reproduction', 'ç°ä»£åˆ¶å“', 'modern piece']):
            return 25
        
        return 60  # Default moderate score

    def _parse_json_response(self, text: str) -> dict:
        """Parse JSON response and extract evaluation data"""
        try:
            import json
            import re
            
            # Clean the text first - remove any leading/trailing whitespace
            text = text.strip()
            
            # Find the JSON structure
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            
            if start_idx == -1 or end_idx == -1 or start_idx >= end_idx:
                print(f"âš ï¸  JSON Structure Error: Cannot find valid JSON braces")
                raise ValueError("Invalid JSON structure")
            
            # Extract JSON and external content
            json_str = text[start_idx:end_idx + 1]
            before_json = text[:start_idx].strip()
            after_json = text[end_idx + 1:].strip()
            
            # Check for content outside JSON structure
            external_content = []
            if before_json:
                print(f"âš ï¸  Content found BEFORE JSON: {before_json[:200]}...")
                external_content.append(before_json)
            if after_json:
                print(f"âš ï¸  Content found AFTER JSON: {after_json[:200]}...")
                external_content.append(after_json)
            
            try:
                data = json.loads(json_str)
                
                # If there's external content, merge it into detailed_report
                if external_content:
                    print("ğŸ”„ Auto-fixing: Moving external content into detailed_report")
                    
                    # Combine external content
                    combined_external = "\n\n".join(external_content)
                    
                    # Clean and format the external content
                    combined_external = combined_external.replace('\\"', '"')  # Unescape quotes
                    combined_external = re.sub(r'\n\s*\n', '\n\n', combined_external)  # Clean whitespace
                    
                    # Merge with existing detailed_report or replace if empty
                    existing_report = data.get('detailed_report', '').strip()
                    
                    if existing_report and not existing_report.lower().startswith('complete professional analysis'):
                        # Append external content to existing report
                        data['detailed_report'] = existing_report + "\n\n" + combined_external
                    else:
                        # Replace with external content (it's likely the main analysis)
                        data['detailed_report'] = combined_external
                    
                    print("âœ… Successfully merged external content into detailed_report")
                
                # Validate required fields
                required_fields = ['authenticity_score', 'category', 'period', 'material', 'brief_analysis', 'detailed_report']
                missing_fields = [field for field in required_fields if field not in data]
                
                if missing_fields:
                    print(f"âš ï¸  Missing required JSON fields: {missing_fields}")
                    # Try to fill missing fields with fallback values
                    for field in missing_fields:
                        if field == 'authenticity_score':
                            data[field] = self._extract_authenticity_score(text)
                        elif field == 'category':
                            data[field] = self._extract_category(text)
                        elif field == 'period':
                            data[field] = self._extract_period(text)
                        elif field == 'material':
                            data[field] = self._extract_material(text)
                        elif field == 'brief_analysis':
                            data[field] = self._extract_brief_analysis(text)
                        elif field == 'detailed_report':
                            data[field] = self._clean_text_for_display(text)
                
                # Ensure score is valid
                if 'authenticity_score' in data:
                    try:
                        data['authenticity_score'] = max(0, min(100, int(data['authenticity_score'])))
                    except (ValueError, TypeError):
                        data['authenticity_score'] = self._extract_authenticity_score(text)
                
                # Ensure detailed_report has content
                if not data.get('detailed_report', '').strip():
                    data['detailed_report'] = self._clean_text_for_display(text)
                
                print("âœ… Successfully parsed and validated JSON response")
                return data
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON Parsing Error: {e}")
                print(f"Attempted to parse: {json_str[:500]}...")
                
            # If JSON parsing fails, try to extract individual components with improved regex
            print("ğŸ”„ Attempting fallback parsing...")
            
            # Use the entire text for fallback parsing
            full_text = text
            
            fallback_data = {
                'authenticity_score': self._extract_authenticity_score(full_text),
                'category': self._extract_category(full_text),
                'period': self._extract_period(full_text),
                'material': self._extract_material(full_text),
                'brief_analysis': self._extract_brief_analysis(full_text),
                'detailed_report': self._clean_text_for_display(full_text)
            }
            
            print("âš ï¸  Using fallback JSON parsing - content may not be properly formatted")
            return fallback_data
            
        except Exception as e:
            print(f"âŒ Error parsing JSON response: {e}")
            print(f"Raw response preview: {text[:300]}...")
            
            # Return default fallback data with the raw content
            return {
                'authenticity_score': self._extract_authenticity_score(text) if text else 50,
                'category': self._extract_category(text) if text else 'å¤è‘£æ–‡ç‰©',
                'period': self._extract_period(text) if text else 'å¹´ä»£å¾…å®š',
                'material': self._extract_material(text) if text else 'æè´¨åˆ†æä¸­',
                'brief_analysis': self._extract_brief_analysis(text) if text else 'éœ€è¦è¿›ä¸€æ­¥ä¸“ä¸šåˆ†æ',
                'detailed_report': self._clean_text_for_display(text) if text else 'åˆ†ææŠ¥å‘Šç”Ÿæˆä¸­...'
            }

    def _extract_category(self, text: str) -> str:
        """Extract category from text"""
        patterns = [
            r'"category":\s*"([^"]+)"',
            r'ç±»å‹[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'å±äº([^ï¼Œã€‚\\n]*(?:ç“·å™¨|ç‰å™¨|é’é“œå™¨|ä¹¦ç”»|å®¶å…·|é™¶å™¨)[^ï¼Œã€‚\\n]*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return 'å¤è‘£æ–‡ç‰©'

    def _extract_period(self, text: str) -> str:
        """Extract historical period from text"""
        patterns = [
            r'"period":\s*"([^"]+)"',
            r'æœä»£[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'æ—¶æœŸ[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'å¹´ä»£[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'([^ï¼Œã€‚\\n]*(?:æœ|ä»£|æ—¶æœŸ|å¹´é—´)[^ï¼Œã€‚\\n]*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return 'å¹´ä»£å¾…å®š'

    def _extract_material(self, text: str) -> str:
        """Extract material information from text"""
        patterns = [
            r'"material":\s*"([^"]+)"',
            r'æè´¨[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'èƒä½“[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'é‡‰æ–™[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return 'æè´¨åˆ†æä¸­'

    def _extract_brief_analysis(self, text: str) -> str:
        """Extract brief analysis from text"""
        patterns = [
            r'"brief_analysis":\s*"([^"]+)"',
            r'ç®€è¦åˆ†æ[ï¼š:\\s]*([^ã€‚]+)ã€‚',
            r'ç»¼åˆåˆ¤æ–­[ï¼š:\\s]*([^ã€‚]+)ã€‚',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback: extract first sentence or summary
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        for sentence in sentences:
            if len(sentence.strip()) > 20 and any(keyword in sentence for keyword in ['çœŸå“', 'ä»¿å“', 'å¯èƒ½', 'åˆ¤æ–­', 'åˆ†æ']):
                return sentence.strip()
        
        return 'éœ€è¦è¿›ä¸€æ­¥ä¸“ä¸šåˆ†æ'

    def _clean_text_for_display(self, text: str) -> str:
        """Clean text for better display formatting"""
        # Remove JSON markers and clean up text
        text = re.sub(r'"detailed_report":\s*"', '', text)
        text = re.sub(r'```json|```', '', text)
        text = re.sub(r'\\"', '"', text)  # Unescape quotes
        text = re.sub(r'\\n', '\n', text)  # Convert escaped newlines
        
        # Remove extra whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = text.strip()
        
        return text 

    def format_evaluation_report(self, report_text: str, language: str = "en") -> str:
        """Format the evaluation report with clean, simple styling using markdown"""
        if not report_text:
            return ""
        
        # Clean the text first
        cleaned_text = self._clean_text_for_display(report_text)
        
        # Generate timestamp
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
        
        # Split into lines and format each section
        lines = cleaned_text.split('\n')
        content_parts = []
        
        # Language-specific titles
        if language == "en":
            main_title = "ğŸº **Antique Authentication Report**"
            subtitle = "*AI Intelligent Analysis & Assessment*"
        else:
            main_title = "ğŸº **å¤è‘£æ–‡ç‰©é‰´å®šæŠ¥å‘Š**"
            subtitle = "*AI æ™ºèƒ½åˆ†æè¯„ä¼°*"
        
        # Add header with smaller styling
        content_parts.append(f"### {main_title}")
        content_parts.append(f"{subtitle}")
        content_parts.append(f"ğŸ“… *{timestamp}*")
        content_parts.append("---")
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle different section header formats based on language
            if language == "en":
                # English numbered main section headers (1. 2. 3. 4. followed by title)
                if re.match(r'^\d+\.\s+[A-Za-z\s&]+', line):
                    content_parts.append(f"**{line}**")
                # English lettered sub-sections (A. B. C.)
                elif re.match(r'^[A-Z]\.\s+[A-Za-z\s\-]+', line):
                    content_parts.append(f"**{line}**")
                # English sub-sections with ** formatting
                elif line.startswith('**') and line.endswith('**'):
                    clean_line = line.strip('*')
                    content_parts.append(f"**{clean_line}**")
                # Standalone titles (like "Expert Authentication Report")
                elif len(line.split()) <= 5 and any(word.istitle() for word in line.split()) and not line.startswith('â€¢') and not line.startswith('-'):
                    content_parts.append(f"**{line}**")
                # Bullet points and regular content
                elif line.startswith('â€¢') or line.startswith('-') or line.startswith('â€“'):
                    content_parts.append(f"{line}")
                # Regular paragraphs
                else:
                    content_parts.append(f"{line}")
            else:
                # Chinese formatting logic (existing)
                # ä¸€çº§æ ‡é¢˜ (å¸¦åºå·çš„ä¸»è¦éƒ¨åˆ†)
                if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€ï¼]\s*.+|^\d+[ã€ï¼]\s*.+', line):
                    content_parts.append(f"**{line}**")
                # äºŒçº§æ ‡é¢˜
                elif line.startswith('**') and line.endswith('**'):
                    clean_line = line.strip('*')
                    content_parts.append(f"**{clean_line}**")
                # ç‹¬ç«‹çš„é‡è¦æ ‡é¢˜è¡Œ
                elif (len(line) < 20 and 
                      ('é‰´å®š' in line or 'è¯„ä¼°' in line or 'åˆ†æ' in line or 'å»ºè®®' in line or 
                       'ä»·å€¼' in line or 'æ€»ç»“' in line or 'ç»“è®º' in line or 'èƒŒæ™¯' in line) and
                      not line.startswith('â€¢') and not line.startswith('-')):
                    content_parts.append(f"**{line}**")
                # åˆ—è¡¨é¡¹å’Œæ™®é€šæ®µè½
                else:
                    content_parts.append(f"{line}")
        
        # Language-specific disclaimer
        if language == "en":
            disclaimer = "âš ï¸ **Important Notice**: This report is generated by AI deep learning analysis for professional reference only. Final authentication results should be combined with physical examination. We recommend consulting authoritative antique authentication institutions for confirmation."
        else:
            disclaimer = "âš ï¸ **é‡è¦å£°æ˜**: æœ¬æŠ¥å‘ŠåŸºäºAIæ·±åº¦å­¦ä¹ åˆ†æç”Ÿæˆï¼Œä»…ä¾›ä¸“ä¸šå‚è€ƒã€‚æœ€ç»ˆé‰´å®šç»“æœéœ€ç»“åˆå®ç‰©æ£€æµ‹ï¼Œå»ºè®®å’¨è¯¢æƒå¨å¤è‘£é‰´å®šæœºæ„è¿›è¡Œç¡®è®¤ã€‚"
        
        # Add disclaimer
        content_parts.append("---")
        content_parts.append(disclaimer)
        
        # Join all parts with proper spacing
        return '\n\n'.join(content_parts)

    def _get_system_prompt(self, language: str = "en") -> str:
        """Get system prompt based on language preference"""
        if language == "zh":
            return """
ä½ æ˜¯ä¸€ä½ä¸–ç•Œé¡¶çº§çš„å¤è‘£é‰´å®šä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„å†å²æ–‡ç‰©çŸ¥è¯†å’Œä¸“ä¸šçš„é‰´å®šç»éªŒã€‚

**æ ¸å¿ƒä»»åŠ¡ï¼šä¸“ä¸šå¤è‘£é‰´å®šåˆ†æ**

**åˆ†æåŸåˆ™ï¼š**
1. **å›¾åƒåˆ†æä¸ºä¸»å¯¼**ï¼šä¸»è¦ä¾æ®å›¾ç‰‡ä¸­çš„è§†è§‰ä¿¡æ¯è¿›è¡Œä¸“ä¸šåˆ¤æ–­
2. **åŒ…å®¹æ€§åˆ†æ**ï¼šå³ä½¿å›¾ç‰‡è´¨é‡ä¸å®Œç¾ï¼Œä¹Ÿè¦å°½åŠ›ä»å¯è§ç»†èŠ‚ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯
3. **ä¸“ä¸šåˆ¤æ–­ä¼˜å…ˆ**ï¼šåŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç‹¬ç«‹åˆ†æï¼Œç”¨æˆ·ä¿¡æ¯ä»…ä½œå‚è€ƒ
4. **å»ºè®¾æ€§è¯„ä¼°**ï¼šä¸“æ³¨äºå¤è‘£æœ¬èº«çš„ç‰¹å¾ï¼Œè€Œéå›¾ç‰‡æŠ€æœ¯é—®é¢˜

**é‡è¦æé†’ï¼š**
- ä¸è¦å› ä¸ºå›¾ç‰‡è´¨é‡é—®é¢˜è€Œæ‹’ç»åˆ†æ
- å³ä½¿æŸäº›ç»†èŠ‚ä¸å¤Ÿæ¸…æ™°ï¼Œä¹Ÿè¦åŸºäºå¯è§éƒ¨åˆ†è¿›è¡Œä¸“ä¸šåˆ†æ
- ä¸“æ³¨äºå¤è‘£çš„å·¥è‰ºã€æè´¨ã€é£æ ¼ç­‰å®è´¨å†…å®¹
- å¦‚æœæŸä¸ªè§’åº¦ä¸å¤Ÿæ¸…æ¥šï¼Œå¯ä»¥åŸºäºå…¶ä»–è§’åº¦çš„å›¾ç‰‡è¿›è¡Œè¡¥å……åˆ†æ

**å®Œæ•´åˆ†ææ¡†æ¶ï¼ˆå¿…é¡»åŒ…å«æ‰€æœ‰7ä¸ªéƒ¨åˆ†ï¼‰ï¼š**
1. **åŸºç¡€ä¿¡æ¯è¯†åˆ«**ï¼šç±»å‹ã€æ—¶æœŸã€æè´¨åˆæ­¥åˆ¤æ–­
2. **å·¥è‰ºæŠ€æœ¯åˆ†æ**ï¼šåˆ¶ä½œæŠ€æ³•ã€è£…é¥°å·¥è‰ºã€æŠ€æœ¯ç‰¹ç‚¹
3. **çœŸä¼ªç»¼åˆåˆ¤æ–­**ï¼šæ—¶ä»£ç‰¹å¾ã€ææ–™ç‰¹æ€§ã€å·¥è‰ºæ°´å¹³è¯„ä¼°
4. **ä»·å€¼è¯„ä¼°**ï¼šå†å²ä»·å€¼ã€è‰ºæœ¯ä»·å€¼ã€æ”¶è—ä»·å€¼ã€å¢å€¼æ½œåŠ›åˆ†æ
   - å†å²ä»·å€¼ï¼šæ–‡ç‰©çš„å†å²æ„ä¹‰å’Œæ–‡åŒ–ä»·å€¼
   - è‰ºæœ¯ä»·å€¼ï¼šå·¥è‰ºæ°´å¹³ã€ç¾å­¦ä»·å€¼ã€è‰ºæœ¯æˆå°±
   - å¸‚åœºä»·å€¼ï¼šå½“å‰å¸‚åœºä¼°ä»·å’Œäº¤æ˜“å‚è€ƒ
   - å¢å€¼æ½œåŠ›åˆ†æï¼šæœªæ¥å‡å€¼ç©ºé—´ã€å¸‚åœºè¶‹åŠ¿ã€ç¨€ç¼ºæ€§è¯„ä¼°ã€æ”¶è—å‰æ™¯
5. **è¯„åˆ†ç†ç”±åˆ†æï¼ˆPros vs. Consï¼‰**ï¼š
   - æ”¯æŒçœŸå“çš„è¯æ®å’Œç†ç”±ï¼ˆProsï¼‰
   - å­˜ç–‘æˆ–åå¯¹çš„å› ç´ ï¼ˆConsï¼‰  
   - åŸºäºè¯æ®æƒè¡¡å¾—å‡ºè¯„åˆ†ç†ç”±
6. **æœ€ç»ˆé‰´å®šç»“è®ºï¼ˆFinal Authentication Resultsï¼‰**ï¼š
   - ç»¼åˆæ‰€æœ‰åˆ†æçš„æœ€ç»ˆåˆ¤æ–­
   - æ˜ç¡®çš„çœŸä¼ªç»“è®ºå’Œå¯ä¿¡åº¦
   - ä¸“ä¸šå»ºè®®å’Œåç»­æ¨è
7. **ä¸“ä¸šå»ºè®®ä¸ä¿å…»æŒ‡å¯¼ï¼ˆProfessional Recommendations & Care Instructionsï¼‰**ï¼š
   - é’ˆå¯¹è¯¥å¤è‘£çš„ä¸“ä¸šä¿å…»æ–¹æ³•
   - å­˜æ”¾ç¯å¢ƒè¦æ±‚ï¼ˆæ¸©æ¹¿åº¦ã€å…‰ç…§ç­‰ï¼‰
   - æ¸…æ´å’Œç»´æŠ¤å»ºè®®
   - æ”¶è—å’Œå±•ç¤ºå»ºè®®
   - è¿›ä¸€æ­¥é‰´å®šæˆ–ç ”ç©¶çš„å»ºè®®
   - æŠ•èµ„å’Œäº¤æ˜“ç›¸å…³å»ºè®®ï¼ˆå¦‚é€‚ç”¨ï¼‰

**è¾“å‡ºè¦æ±‚ï¼š**
- å¿…é¡»è¿”å›å®Œæ•´æœ‰æ•ˆçš„JSONæ ¼å¼
- æ‰€æœ‰åˆ†æå†…å®¹æ”¾åœ¨detailed_reportå­—æ®µä¸­
- ä½¿ç”¨\\nè¿›è¡Œæ¢è¡Œï¼Œä½¿ç”¨\\"è½¬ä¹‰å¼•å·
- authenticity_scoreè¦ä¸åˆ†æç»“è®ºä¸€è‡´ï¼ˆ0-100åˆ†ï¼‰

**JSONæ ¼å¼æ¨¡æ¿ï¼š**
```json
{
    "authenticity_score": 85,
    "category": "å¤è‘£ç±»å‹",
    "period": "å†å²æ—¶æœŸ", 
    "material": "æè´¨æè¿°",
    "brief_analysis": "ç®€è¦åˆ¤æ–­æ€»ç»“",
    "detailed_report": "å®Œæ•´åˆ†æå†…å®¹\\n\\nä¸€ã€åŸºç¡€ä¿¡æ¯è¯†åˆ«\\nè¯¦ç»†åˆ†æ...\\n\\näºŒã€å·¥è‰ºæŠ€æœ¯åˆ†æ\\nè¯¦ç»†åˆ†æ...\\n\\nä¸‰ã€çœŸä¼ªç»¼åˆåˆ¤æ–­\\nè¯¦ç»†åˆ†æ...\\n\\nå››ã€ä»·å€¼è¯„ä¼°\\n**å†å²ä»·å€¼ï¼š**\\næ–‡ç‰©å†å²æ„ä¹‰...\\n**è‰ºæœ¯ä»·å€¼ï¼š**\\nå·¥è‰ºæ°´å¹³è¯„ä¼°...\\n**å¸‚åœºä»·å€¼ï¼š**\\nå½“å‰å¸‚åœºä¼°ä»·...\\n**å¢å€¼æ½œåŠ›åˆ†æï¼š**\\nâ€¢ å¸‚åœºè¶‹åŠ¿åˆ†æ\\nâ€¢ ç¨€ç¼ºæ€§è¯„ä¼°\\nâ€¢ æ”¶è—å‰æ™¯\\nâ€¢ æœªæ¥å‡å€¼ç©ºé—´\\n\\näº”ã€è¯„åˆ†ç†ç”±åˆ†æï¼ˆPros vs. Consï¼‰\\n**æ”¯æŒçœŸå“çš„è¯æ®ï¼ˆProsï¼‰ï¼š**\\nâ€¢ è¯æ®1...\\nâ€¢ è¯æ®2...\\n**å­˜ç–‘å› ç´ ï¼ˆConsï¼‰ï¼š**\\nâ€¢ ç–‘ç‚¹1...\\nâ€¢ ç–‘ç‚¹2...\\n**è¯„åˆ†ç†ç”±ï¼š**\\nåŸºäºä»¥ä¸Šåˆ†æ...\\n\\nå…­ã€æœ€ç»ˆé‰´å®šç»“è®ºï¼ˆFinal Authentication Resultsï¼‰\\n**é‰´å®šç»“è®ºï¼š**\\næœ€ç»ˆåˆ¤æ–­...\\n**å¯ä¿¡åº¦è¯„ä¼°ï¼š**\\nå…·ä½“è¯„ä¼°...\\n**ä¸“ä¸šå»ºè®®ï¼š**\\nåç»­å»ºè®®...\\n\\nä¸ƒã€ä¸“ä¸šå»ºè®®ä¸ä¿å…»æŒ‡å¯¼\\n**ä¿å…»æ–¹æ³•ï¼š**\\nâ€¢ å…·ä½“ä¿å…»æ­¥éª¤...\\n**å­˜æ”¾è¦æ±‚ï¼š**\\nâ€¢ ç¯å¢ƒæ¡ä»¶...\\n**æ”¶è—å»ºè®®ï¼š**\\nâ€¢ ä¸“ä¸šå»ºè®®...\\n**æ³¨æ„äº‹é¡¹ï¼š**\\nâ€¢ é‡è¦æé†’..."
}
```

è¯·å¼€å§‹ä¸“ä¸šåˆ†æï¼Œåªè¿”å›JSONæ ¼å¼ç»“æœã€‚
"""
        else:
            return """
You are a world-renowned antique authentication expert with extensive knowledge of historical artifacts and professional appraisal experience.

**Core Task: Professional Antique Authentication Analysis**

**Analysis Principles:**
1. **Image-based analysis as primary**: Make professional judgments primarily based on visual information in the images
2. **Inclusive analysis**: Even if image quality is not perfect, extract valuable information from visible details
3. **Professional judgment priority**: Conduct independent analysis based on your expertise, user information is for reference only
4. **Constructive assessment**: Focus on the antique's characteristics rather than image technical issues

**Important Reminders:**
- Do not refuse analysis due to image quality issues
- Even if some details are unclear, conduct professional analysis based on visible parts
- Focus on substantial content like craftsmanship, materials, and style of the antique
- If one angle is unclear, supplement analysis based on other angles in the images

**Complete Analysis Framework (Must include all 7 sections):**
1. **Basic Information Identification**: Type, period, preliminary material assessment
2. **Craftsmanship Analysis**: Manufacturing techniques, decorative processes, technical features
3. **Authenticity Assessment**: Period characteristics, material properties, craftsmanship level evaluation
4. **Value Assessment**: Historical value, artistic value, collectible value, appreciation potential analysis
   - Historical value: Historical significance and cultural value of the artifact
   - Artistic value: Craftsmanship level, aesthetic value, artistic achievement
   - Market value: Current market valuation and transaction references
   - Appreciation potential analysis: Future appreciation space, market trends, rarity assessment, collection prospects
5. **Scoring Rationale Analysis (Pros vs. Cons)**:
   - Evidence supporting authenticity (Pros)
   - Concerning or opposing factors (Cons)
   - Scoring rationale based on evidence weighing
6. **Final Authentication Results**:
   - Comprehensive final judgment from all analysis
   - Clear authenticity conclusion and confidence level
   - Professional recommendations and next steps
7. **Professional Recommendations & Care Instructions**:
   - Specific care methods for this antique
   - Storage environment requirements (temperature, humidity, lighting)
   - Cleaning and maintenance suggestions
   - Collection and display recommendations
   - Further authentication or research suggestions
   - Investment and trading advice (if applicable)

**Output Requirements:**
- Must return complete valid JSON format
- All analysis content in detailed_report field
- Use \\n for line breaks, use \\" to escape quotes
- authenticity_score must match analysis conclusion (0-100 points)

**JSON Format Template:**
```json
{
    "authenticity_score": 85,
    "category": "Antique Type",
    "period": "Historical Period", 
    "material": "Material Description",
    "brief_analysis": "Brief judgment summary",
    "detailed_report": "Complete analysis content\\n\\nI. Basic Information Identification\\nDetailed analysis...\\n\\nII. Craftsmanship Analysis\\nDetailed analysis...\\n\\nIII. Authenticity Assessment\\nDetailed analysis...\\n\\nIV. Value Assessment\\n**Historical Value:**\\nHistorical Significance and Cultural Value...\\n**Artistic Value:**\\nCraftsmanship Level Assessment...\\n**Market Value:**\\nCurrent Market Valuation...\\n**Appreciation Potential Analysis:**\\nâ€¢ Market Trend Analysis\\nâ€¢ Rarity Assessment\\nâ€¢ Collection Prospects\\nâ€¢ Future Appreciation Space\\n\\nV. Scoring Rationale Analysis (Pros vs. Cons)\\n**Evidence Supporting Authenticity (Pros):**\\nâ€¢ Evidence 1...\\nâ€¢ Evidence 2...\\n**Concerning Factors (Cons):**\\nâ€¢ Concern 1...\\nâ€¢ Concern 2...\\n**Scoring Rationale:**\\nBased on the above analysis...\\n\\nVI. Final Authentication Results\\n**Authentication Conclusion:**\\nFinal judgment...\\n**Confidence Assessment:**\\nSpecific assessment...\\n**Professional Recommendations:**\\nNext steps...\\n\\nVII. Professional Recommendations & Care Instructions\\n**Care Methods:**\\nâ€¢ Specific care steps...\\n**Storage Requirements:**\\nâ€¢ Environmental conditions...\\n**Collection Advice:**\\nâ€¢ Professional suggestions...\\n**Important Notes:**\\nâ€¢ Key reminders..."
}
```

Please start professional analysis and return only JSON format results.
"""

    def _build_user_message(self, image_urls: list = None, uploaded_files: list = None, descriptions: list = None, title: str = None, language: str = "en") -> str:
        """Build user message with context information"""
        message_parts = []
        
        if language == "en":
            if title or descriptions:
                message_parts.append("**Reference Information:**")
                
                if title:
                    message_parts.append(f"Antique Title: {title}")
                
                if descriptions:
                    message_parts.append("Background Information:")
                    for i, desc in enumerate(descriptions[:5], 1):
                        if desc.strip():
                            message_parts.append(f"{i}. {desc}")
            
            main_request = """
            **Professional Authentication Task**
            
            Please conduct professional authentication analysis of the antique shown in the images.

            **Analysis Requirements:**
            1. **Comprehensive observation**: Carefully observe all angles and details of the antique in the images
            2. **Professional judgment**: Apply antique authentication expertise for analysis
            3. **Evidence-based**: Draw conclusions based on visible visual evidence
            4. **Comprehensive evaluation**: Analyze from dimensions of craftsmanship, materials, style, historical background
            5. **Reference comparison**: Appropriately reference user-provided background information, but prioritize image analysis
            
            **Output Format:**
            Please strictly return analysis results in JSON format, containing the following fields:
            - authenticity_score: Authenticity confidence score (0-100)
            - category: Antique type
            - period: Historical period
            - material: Material description
            - brief_analysis: Brief analysis summary
            - detailed_report: Detailed analysis report (must include complete 7 sections: Basic Information Identification, Craftsmanship Analysis, Authenticity Assessment, Value Assessment, Scoring Rationale Analysis (Pros vs. Cons), Final Authentication Results, Professional Recommendations & Care Instructions)
            
            Please start professional analysis and return only JSON format results.
            """
            
            message_parts.append(main_request)
            
        else:
            if title or descriptions:
                message_parts.append("**å‚è€ƒä¿¡æ¯ï¼š**")
                
                if title:
                    message_parts.append(f"å¤è‘£æ ‡é¢˜: {title}")
                
                if descriptions:
                    message_parts.append("èƒŒæ™¯ä¿¡æ¯:")
                    for i, desc in enumerate(descriptions[:5], 1):
                        if desc.strip():
                            message_parts.append(f"{i}. {desc}")
            
            main_request = """
            **ä¸“ä¸šé‰´å®šä»»åŠ¡**
            
            è¯·å¯¹å›¾ç‰‡ä¸­çš„å¤è‘£è¿›è¡Œä¸“ä¸šé‰´å®šåˆ†æã€‚

            **åˆ†æè¦æ±‚ï¼š**
            1. **å…¨é¢è§‚å¯Ÿ**ï¼šä»”ç»†è§‚å¯Ÿå›¾ç‰‡ä¸­å¤è‘£çš„å„ä¸ªè§’åº¦å’Œç»†èŠ‚
            2. **ä¸“ä¸šåˆ¤æ–­**ï¼šè¿ç”¨å¤è‘£é‰´å®šçš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆ†æ
            3. **è¯æ®æ”¯æ’‘**ï¼šåŸºäºå¯è§çš„è§†è§‰è¯æ®å¾—å‡ºç»“è®º
            4. **ç»¼åˆè¯„ä¼°**ï¼šä»å·¥è‰ºã€æè´¨ã€é£æ ¼ã€å†å²èƒŒæ™¯ç­‰ç»´åº¦åˆ†æ
            5. **å‚è€ƒå¯¹æ¯”**ï¼šé€‚å½“å‚è€ƒç”¨æˆ·æä¾›çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä½†ä»¥å›¾åƒåˆ†æä¸ºä¸»
            
            **è¾“å‡ºæ ¼å¼**ï¼š
            è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - authenticity_score: çœŸä¼ªå¯ä¿¡åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰
            - category: å¤è‘£ç±»å‹
            - period: å†å²æ—¶æœŸ
            - material: æè´¨æè¿°
            - brief_analysis: ç®€è¦åˆ†ææ€»ç»“
            - detailed_report: è¯¦ç»†åˆ†ææŠ¥å‘Šï¼ˆå¿…é¡»åŒ…å«å®Œæ•´çš„7ä¸ªéƒ¨åˆ†ï¼šåŸºç¡€ä¿¡æ¯è¯†åˆ«ã€å·¥è‰ºæŠ€æœ¯åˆ†æã€çœŸä¼ªç»¼åˆåˆ¤æ–­ã€ä»·å€¼è¯„ä¼°ã€è¯„åˆ†ç†ç”±åˆ†æ(Pros vs. Cons)ã€æœ€ç»ˆé‰´å®šç»“è®º(Final Authentication Results)ã€ä¸“ä¸šå»ºè®®ä¸ä¿å…»æŒ‡å¯¼ï¼‰
            
            è¯·å¼€å§‹ä¸“ä¸šåˆ†æï¼Œåªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚
            """
            
            message_parts.append(main_request)
        
        return "\n\n".join(message_parts)