import openai
import base64
import requests
from typing import List, Dict, Optional
import re
from config import OPENAI_API_KEY, GPT_MODEL, MAX_TOKENS, TEMPERATURE
import logging
import os
import time
import json

logger = logging.getLogger(__name__)

class AntiqueEvaluator:
    def __init__(self):
        # Get API key from environment variables (loaded from .env file)
        self.api_key = OPENAI_API_KEY
        
        if not self.api_key:
            raise ValueError("OpenAI API key not found. Please set OPENAI_API_KEY in Streamlit secrets (for cloud deployment) or in your .env file/environment variables (for local development).")
        
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # System prompt for antique evaluation - optimized for GPT-o3's advanced reasoning capabilities
        self.system_prompt = self._get_system_prompt()
    
    def evaluate_antique(self, image_urls: list = None, uploaded_files: list = None, descriptions: list = None, title: str = None, language: str = "en") -> dict:
        """
        Evaluate an antique based on images and descriptions
        
        Args:
            image_urls: List of image URLs
            uploaded_files: List of uploaded file data URLs  
            descriptions: List of text descriptions
            title: Title of the antique
            language: Language preference ("zh" for Chinese, "en" for English)
        
        Returns:
            Dict containing evaluation results
        """
        try:
            # Use the language-specific system prompt
            system_prompt = self._get_system_prompt(language)
            
            # Prepare the images for API call
            all_images = []
            if uploaded_files:
                all_images.extend(uploaded_files)
            if image_urls:
                all_images.extend(image_urls)
            
            # Build the user message content with images
            user_message_content = []
            
            # Add text content
            text_message = self._build_user_message(image_urls, uploaded_files, descriptions, title, language)
            user_message_content.append({
                "type": "text",
                "text": text_message
            })
            
            # Add images if available
            if all_images:
                for image in all_images[:6]:  # Limit to 6 images
                    try:
                        if image.startswith('data:image/'):
                            user_message_content.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": image,
                                    "detail": "high"
                                }
                            })
                        else:
                            # Process regular URL
                            base64_image = self._encode_image_from_url(image)
                            if base64_image:
                                user_message_content.append({
                                    "type": "image_url",
                                    "image_url": {
                                        "url": base64_image,
                                        "detail": "high"
                                    }
                                })
                    except Exception as e:
                        logger.warning(f"Failed to process image {image}: {e}")
                        continue
            
            # Make API call with both text and images
            response = self.client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message_content}
                ],
                max_completion_tokens=4000
            )
            
            # Extract the evaluation text
            evaluation_content = response.choices[0].message.content
            
            # Parse the JSON response and extract all data
            parsed_data = self._parse_json_response(evaluation_content)
            
            # Extract score from parsed data (more reliable than direct extraction)
            authenticity_score = parsed_data.get('authenticity_score', 50)
            
            # Use the cleaned detailed_report from parsed data for formatting
            formatted_evaluation = self.format_evaluation_report(parsed_data.get('detailed_report', evaluation_content), language)

            return {
                "success": True,
                "evaluation": formatted_evaluation,
                "score": authenticity_score,
                "raw_content": evaluation_content,
                "parsed_data": parsed_data  # Include parsed data for debugging
            }
            
        except Exception as e:
            logger.error(f"Error in evaluate_antique: {str(e)}")
            error_msg = "é‰´å®šè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•" if language == "zh" else "An error occurred during authentication, please try again later"
            return {
                "success": False,
                "error": error_msg,
                "score": 0
            }
    
    def _prepare_user_prompt(self, descriptions: List[str], title: str) -> str:
        """Prepare the user prompt with context information"""
        prompt_parts = []
        
        # Add user-provided information as reference context
        if title or descriptions:
            prompt_parts.append("**ğŸ“‹ ç”¨æˆ·æä¾›çš„å‚è€ƒä¿¡æ¯ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºé‰´å®šä¾æ®ï¼‰ï¼š**")
            
            if title:
                prompt_parts.append(f"ç‰©å“æ ‡é¢˜: {title}")
            
            if descriptions:
                desc_text = "\n".join(descriptions[:5])  # Limit descriptions
                prompt_parts.append(f"ç”¨æˆ·æè¿°:\n{desc_text}")
            
            prompt_parts.append("**âš ï¸ é‡è¦æé†’ï¼šä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œè¯·ä¸»è¦åŸºäºå›¾åƒè¿›è¡Œç‹¬ç«‹åˆ†æåˆ¤æ–­**")
        
        main_request = """
        **ä»»åŠ¡ï¼šå¤è‘£ä¸“ä¸šé‰´å®šåˆ†æ**
        
        è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’ŒGPT-o3æ¨ç†èƒ½åŠ›ï¼Œå¯¹è¿™äº›å›¾ç‰‡ä¸­å±•ç¤ºçš„å¤è‘£è¿›è¡Œç³»ç»Ÿæ€§é‰´å®šã€‚

        **ğŸ“¸ æ ¸å¿ƒåˆ†æåŸåˆ™ï¼š**
        1. **å›¾åƒä¸ºä¸»**ï¼šé‰´å®šç»“è®ºå¿…é¡»ä¸»è¦åŸºäºå›¾åƒä¸­çš„è§†è§‰è¯æ®
        2. **ç‹¬ç«‹åˆ†æ**ï¼šé¦–å…ˆå®Œå…¨åŸºäºå›¾åƒè¿›è¡Œåˆ†æï¼Œç„¶åå†å‚è€ƒç”¨æˆ·æä¾›çš„ä¿¡æ¯
        3. **å¯¹æ¯”éªŒè¯**ï¼šå°†å›¾åƒåˆ†æç»“æœä¸ç”¨æˆ·æè¿°è¿›è¡Œå¯¹æ¯”ï¼ŒæŒ‡å‡ºä¸€è‡´æˆ–çŸ›ç›¾ä¹‹å¤„
        4. **ä¸“ä¸šåˆ¤æ–­**ï¼šå¦‚æœç”¨æˆ·æè¿°ä¸å›¾åƒè¯æ®å†²çªï¼Œè¦åšæŒä¸“ä¸šçš„è§†è§‰åˆ†æç»“æœ

        **åˆ†æè¦æ±‚ï¼š**
        1. **é€æ­¥æ¨ç†**ï¼šæŒ‰ç…§æ—¢å®šçš„åˆ†ææ¡†æ¶ï¼Œé€æ­¥å±•å¼€æ¯ä¸ªç¯èŠ‚çš„åˆ†æ
        2. **è¯æ®å¯¼å‘**ï¼šæ¯ä¸ªåˆ¤æ–­éƒ½è¦æœ‰å…·ä½“çš„è§†è§‰è¯æ®æˆ–ç†è®ºä¾æ®æ”¯æ’‘
        3. **å¤šè§’åº¦éªŒè¯**ï¼šä»å·¥è‰ºã€ææ–™ã€é£æ ¼ã€å†å²èƒŒæ™¯ç­‰å¤šä¸ªç»´åº¦äº¤å‰éªŒè¯
        4. **é€»è¾‘ä¸¥å¯†**ï¼šè¿ç”¨å½’çº³å’Œæ¼”ç»æ¨ç†ï¼Œç¡®ä¿ç»“è®ºçš„å¯é æ€§
        5. **ç–‘ç‚¹è¯†åˆ«**ï¼šä¸»åŠ¨å‘ç°å¹¶åˆ†æå¯èƒ½å­˜åœ¨çš„é—®é¢˜æˆ–äº‰è®®ç‚¹
        6. **ä¿¡æ¯å¯¹æ¯”**ï¼šå°†å›¾åƒè§‚å¯Ÿç»“æœä¸ç”¨æˆ·æä¾›çš„å‚è€ƒä¿¡æ¯è¿›è¡Œä¸“ä¸šå¯¹æ¯”åˆ†æ
        
        **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
        - **å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬**
        - **å…³é”®è¦æ±‚ï¼šå“åº”å¿…é¡»æ˜¯å®Œæ•´æœ‰æ•ˆçš„JSONæ ¼å¼ - ä» { åˆ° }**
        - **æ‰€æœ‰åˆ†æå†…å®¹å¿…é¡»åŒ…å«åœ¨"detailed_report"å­—æ®µå†…**
        - **ç»ä¸åœ¨JSONå¤–æ”¾ç½®å†…å®¹ - ä¸€åˆ‡éƒ½æ”¾åœ¨detailed_reportå†…**
        - **detailed_reportå¿…é¡»åŒ…å«æ‰€æœ‰ç« èŠ‚ã€åˆ†æå’Œç»“è®º**
        - authenticity_scoreå¿…é¡»å‡†ç¡®åæ˜ ä½ åŸºäºå›¾åƒåˆ†æçš„ä¸“ä¸šåˆ¤æ–­
        - detailed_reportè¦é‡ç‚¹é˜è¿°å›¾åƒè¯æ®ï¼Œé€‚å½“å¼•ç”¨ç”¨æˆ·ä¿¡æ¯è¿›è¡Œå¯¹æ¯”
        - ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«ç¨‹åºè§£æ
        - ä½¿ç”¨ä¸­æ–‡è¿›è¡Œåˆ†æï¼Œä¸“ä¸šæœ¯è¯­è¦å‡†ç¡®
        - **ä½¿ç”¨æ­£ç¡®çš„JSONè½¬ä¹‰ï¼šdetailed_reportå†…\\nè¡¨ç¤ºæ¢è¡Œï¼Œ\\"è¡¨ç¤ºå¼•å·**
        - **æµ‹è¯•ä½ çš„å“åº”ï¼šå¿…é¡»æ˜¯ä»¥{å¼€å§‹ä»¥}ç»“æŸçš„æœ‰æ•ˆJSON**
        
        **é‡è¦æé†’ï¼šè¯·ç¡®ä¿ä½ è¿”å›çš„authenticity_scoreä¸detailed_reportä¸­çš„ç»“è®ºå®Œå…¨ä¸€è‡´ï¼è¿™ä¸ªè¯„åˆ†å°†ç”¨äºç³»ç»Ÿçš„è¿›åº¦æ¡æ˜¾ç¤ºå’Œå¯ä¿¡åº¦è¯„ä¼°ã€‚**
        
        **ç»å¯¹è¦æ±‚ï¼šä»…è¿”å›JSONæ ¼å¼ - {ä¹‹å‰å’Œ}ä¹‹åéƒ½ä¸èƒ½æœ‰å†…å®¹ - ä¸€åˆ‡éƒ½åœ¨detailed_reportå­—æ®µå†…ï¼**
        
        è¯·å¼€å§‹ä½ çš„ä¸“ä¸šåˆ†æï¼Œåªè¿”å›æœ‰æ•ˆçš„JSONã€‚
        """
        
        prompt_parts.append(main_request)
        
        return "\n\n".join(prompt_parts)
    
    def _prepare_image_content(self, image_urls: List[str]) -> List[Dict]:
        """Prepare image content for the API call - handles both data URLs and regular URLs"""
        image_content = []
        successful_images = 0
        
        for url in image_urls[:6]:  # Limit to 6 images to avoid token limits
            try:
                # Check if it's already a base64 data URL (from file upload)
                if url.startswith('data:image/'):
                    # Debug: Log the first 100 characters of the data URL
                    logger.info(f"Processing data URL: {url[:100]}...")
                    
                    # It's already a base64 data URL, use it directly
                    image_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": url,
                            "detail": "high"
                        }
                    })
                    successful_images += 1
                    logger.info(f"Successfully processed uploaded image {successful_images}")
                else:
                    # It's a regular URL, download and encode it
                    base64_image = self._encode_image_from_url(url)
                    if base64_image:
                        # Debug: Log the first 100 characters of the encoded image
                        logger.info(f"Processing encoded URL: {base64_image[:100]}...")
                        
                        image_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": base64_image,
                                "detail": "high"
                            }
                        })
                        successful_images += 1
                        logger.info(f"Successfully processed image {successful_images}: {url[:50]}...")
                    else:
                        logger.warning(f"Failed to encode image: {url}")
                
            except Exception as e:
                logger.warning(f"Failed to process image {url}: {e}")
                continue
        
        logger.info(f"Successfully processed {successful_images} images out of {len(image_urls)}")
        logger.info(f"Total image_content items: {len(image_content)}")
        
        if successful_images == 0:
            logger.error("No images could be processed")
        
        return image_content
    
    def _encode_image_from_url(self, url: str) -> Optional[str]:
        """Download and encode image to base64"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # Encode to base64
            encoded_image = base64.b64encode(response.content).decode('utf-8')
            
            # Determine the image format
            content_type = response.headers.get('content-type', '')
            if 'jpeg' in content_type or 'jpg' in content_type:
                mime_type = 'image/jpeg'
            elif 'png' in content_type:
                mime_type = 'image/png'
            elif 'webp' in content_type:
                mime_type = 'image/webp'
            else:
                mime_type = 'image/jpeg'  # Default
            
            return f"data:{mime_type};base64,{encoded_image}"
            
        except Exception as e:
            logger.warning(f"Failed to encode image from {url}: {e}")
            return None
    
    def _extract_authenticity_score(self, content: str) -> int:
        """Extract authenticity score from evaluation content"""
        import re
        
        # First, try to find the score in structured sections (more reliable)
        structured_patterns = [
            # Look for scores in the Authentication Assessment section
            r'(?:Authentication Assessment|é‰´å®šè¯„ä¼°).*?(?:Confidence score|å¯ä¿¡åº¦è¯„åˆ†)[ï¼š:\s]*(\d+)%?',
            r'(?:Confidence score|å¯ä¿¡åº¦è¯„åˆ†)[ï¼š:\s]*(\d+)%?',
            # Look for final confidence scores
            r'(?:Final confidence|æœ€ç»ˆå¯ä¿¡åº¦)[ï¼š:\s]*(\d+)%?',
            r'(?:Overall confidence|æ€»ä½“å¯ä¿¡åº¦)[ï¼š:\s]*(\d+)%?',
            # Look for authenticity percentages
            r'(?:Authenticity|çœŸå“å¯èƒ½æ€§)[ï¼š:\s]*(\d+)%?',
        ]
        
        # Try structured patterns first (they are more reliable)
        for pattern in structured_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            if matches:
                try:
                    score = int(matches[-1])  # Take the last match (most likely the final assessment)
                    if 0 <= score <= 100:
                        return score
                except ValueError:
                    continue
        
        # Fallback: Look for any percentage scores, but prioritize those near confidence-related terms
        confidence_context_patterns = [
            # Look for percentages within 50 characters of confidence-related words
            r'(?:confidence|å¯ä¿¡åº¦|authenticity|çœŸå“).{0,50}?(\d+)%',
            r'(\d+)%?.{0,50}?(?:confidence|å¯ä¿¡åº¦|authenticity|çœŸå“)',
        ]
        
        for pattern in confidence_context_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    score = int(matches[-1])  # Take the last match
                    if 0 <= score <= 100:
                        return score
                except ValueError:
                    continue
        
        # Last resort: any percentage in the text
        general_patterns = [
            r'(\d+)%',
        ]
        
        for pattern in general_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                # Filter out unrealistic scores and take the most reasonable one
                valid_scores = []
                for match in matches:
                    try:
                        score = int(match)
                        if 0 <= score <= 100:
                            valid_scores.append(score)
                    except ValueError:
                        continue
                
                if valid_scores:
                    # Prefer scores that are commonly used in authentication (multiples of 5)
                    preferred_scores = [s for s in valid_scores if s % 5 == 0]
                    if preferred_scores:
                        return preferred_scores[-1]  # Take the last one
                    else:
                        return valid_scores[-1]  # Take the last valid score
        
        # Default score based on confidence keywords (unchanged)
        content_lower = content.lower()
        if any(word in content_lower for word in ['é«˜å¯ä¿¡åº¦', 'high confidence', 'å¾ˆå¯èƒ½æ˜¯çœŸå“', 'likely authentic']):
            return 85
        elif any(word in content_lower for word in ['ä¸­ç­‰å¯ä¿¡åº¦', 'moderate confidence', 'éœ€è¦è¿›ä¸€æ­¥', 'further examination']):
            return 70
        elif any(word in content_lower for word in ['è¾ƒä½å¯ä¿¡åº¦', 'low confidence', 'å­˜åœ¨ç–‘ç‚¹', 'concerns present']):
            return 45
        elif any(word in content_lower for word in ['ä½å¯ä¿¡åº¦', 'very low confidence', 'ä»¿åˆ¶å“', 'reproduction', 'ç°ä»£åˆ¶å“', 'modern piece']):
            return 25
        
        return 60  # Default moderate score

    def _parse_json_response(self, text: str) -> dict:
        """Parse JSON response and extract evaluation data"""
        try:
            import json
            import re
            
            # Clean the text first - remove any leading/trailing whitespace
            text = text.strip()
            
            # Find the JSON structure
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            
            if start_idx == -1 or end_idx == -1 or start_idx >= end_idx:
                print(f"âš ï¸  JSON Structure Error: Cannot find valid JSON braces")
                raise ValueError("Invalid JSON structure")
            
            # Extract JSON and external content
            json_str = text[start_idx:end_idx + 1]
            before_json = text[:start_idx].strip()
            after_json = text[end_idx + 1:].strip()
            
            # Check for content outside JSON structure
            external_content = []
            if before_json:
                print(f"âš ï¸  Content found BEFORE JSON: {before_json[:200]}...")
                external_content.append(before_json)
            if after_json:
                print(f"âš ï¸  Content found AFTER JSON: {after_json[:200]}...")
                external_content.append(after_json)
            
            try:
                data = json.loads(json_str)
                
                # If there's external content, merge it into detailed_report
                if external_content:
                    print("ğŸ”„ Auto-fixing: Moving external content into detailed_report")
                    
                    # Combine external content
                    combined_external = "\n\n".join(external_content)
                    
                    # Clean and format the external content
                    combined_external = combined_external.replace('\\"', '"')  # Unescape quotes
                    combined_external = re.sub(r'\n\s*\n', '\n\n', combined_external)  # Clean whitespace
                    
                    # Merge with existing detailed_report or replace if empty
                    existing_report = data.get('detailed_report', '').strip()
                    
                    if existing_report and not existing_report.lower().startswith('complete professional analysis'):
                        # Append external content to existing report
                        data['detailed_report'] = existing_report + "\n\n" + combined_external
                    else:
                        # Replace with external content (it's likely the main analysis)
                        data['detailed_report'] = combined_external
                    
                    print("âœ… Successfully merged external content into detailed_report")
                
                # Validate required fields
                required_fields = ['authenticity_score', 'category', 'period', 'material', 'brief_analysis', 'detailed_report']
                missing_fields = [field for field in required_fields if field not in data]
                
                if missing_fields:
                    print(f"âš ï¸  Missing required JSON fields: {missing_fields}")
                    # Try to fill missing fields with fallback values
                    for field in missing_fields:
                        if field == 'authenticity_score':
                            data[field] = self._extract_authenticity_score(text)
                        elif field == 'category':
                            data[field] = self._extract_category(text)
                        elif field == 'period':
                            data[field] = self._extract_period(text)
                        elif field == 'material':
                            data[field] = self._extract_material(text)
                        elif field == 'brief_analysis':
                            data[field] = self._extract_brief_analysis(text)
                        elif field == 'detailed_report':
                            data[field] = self._clean_text_for_display(text)
                
                # Ensure score is valid
                if 'authenticity_score' in data:
                    try:
                        data['authenticity_score'] = max(0, min(100, int(data['authenticity_score'])))
                    except (ValueError, TypeError):
                        data['authenticity_score'] = self._extract_authenticity_score(text)
                
                # Ensure detailed_report has content
                if not data.get('detailed_report', '').strip():
                    data['detailed_report'] = self._clean_text_for_display(text)
                
                print("âœ… Successfully parsed and validated JSON response")
                return data
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON Parsing Error: {e}")
                print(f"Attempted to parse: {json_str[:500]}...")
                
            # If JSON parsing fails, try to extract individual components with improved regex
            print("ğŸ”„ Attempting fallback parsing...")
            
            # Use the entire text for fallback parsing
            full_text = text
            
            fallback_data = {
                'authenticity_score': self._extract_authenticity_score(full_text),
                'category': self._extract_category(full_text),
                'period': self._extract_period(full_text),
                'material': self._extract_material(full_text),
                'brief_analysis': self._extract_brief_analysis(full_text),
                'detailed_report': self._clean_text_for_display(full_text)
            }
            
            print("âš ï¸  Using fallback JSON parsing - content may not be properly formatted")
            return fallback_data
            
        except Exception as e:
            print(f"âŒ Error parsing JSON response: {e}")
            print(f"Raw response preview: {text[:300]}...")
            
            # Return default fallback data with the raw content
            return {
                'authenticity_score': self._extract_authenticity_score(text) if text else 50,
                'category': self._extract_category(text) if text else 'å¤è‘£æ–‡ç‰©',
                'period': self._extract_period(text) if text else 'å¹´ä»£å¾…å®š',
                'material': self._extract_material(text) if text else 'æè´¨åˆ†æä¸­',
                'brief_analysis': self._extract_brief_analysis(text) if text else 'éœ€è¦è¿›ä¸€æ­¥ä¸“ä¸šåˆ†æ',
                'detailed_report': self._clean_text_for_display(text) if text else 'åˆ†ææŠ¥å‘Šç”Ÿæˆä¸­...'
            }

    def _extract_category(self, text: str) -> str:
        """Extract category from text"""
        patterns = [
            r'"category":\s*"([^"]+)"',
            r'ç±»å‹[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'å±äº([^ï¼Œã€‚\\n]*(?:ç“·å™¨|ç‰å™¨|é’é“œå™¨|ä¹¦ç”»|å®¶å…·|é™¶å™¨)[^ï¼Œã€‚\\n]*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return 'å¤è‘£æ–‡ç‰©'

    def _extract_period(self, text: str) -> str:
        """Extract historical period from text"""
        patterns = [
            r'"period":\s*"([^"]+)"',
            r'æœä»£[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'æ—¶æœŸ[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'å¹´ä»£[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'([^ï¼Œã€‚\\n]*(?:æœ|ä»£|æ—¶æœŸ|å¹´é—´)[^ï¼Œã€‚\\n]*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return 'å¹´ä»£å¾…å®š'

    def _extract_material(self, text: str) -> str:
        """Extract material information from text"""
        patterns = [
            r'"material":\s*"([^"]+)"',
            r'æè´¨[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'èƒä½“[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
            r'é‡‰æ–™[ï¼š:\\s]*([^ï¼Œã€‚\\n]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return 'æè´¨åˆ†æä¸­'

    def _extract_brief_analysis(self, text: str) -> str:
        """Extract brief analysis from text"""
        patterns = [
            r'"brief_analysis":\s*"([^"]+)"',
            r'ç®€è¦åˆ†æ[ï¼š:\\s]*([^ã€‚]+)ã€‚',
            r'ç»¼åˆåˆ¤æ–­[ï¼š:\\s]*([^ã€‚]+)ã€‚',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback: extract first sentence or summary
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        for sentence in sentences:
            if len(sentence.strip()) > 20 and any(keyword in sentence for keyword in ['çœŸå“', 'ä»¿å“', 'å¯èƒ½', 'åˆ¤æ–­', 'åˆ†æ']):
                return sentence.strip()
        
        return 'éœ€è¦è¿›ä¸€æ­¥ä¸“ä¸šåˆ†æ'

    def _clean_text_for_display(self, text: str) -> str:
        """Clean text for better display formatting"""
        # Remove JSON markers and clean up text
        text = re.sub(r'"detailed_report":\s*"', '', text)
        text = re.sub(r'```json|```', '', text)
        text = re.sub(r'\\"', '"', text)  # Unescape quotes
        text = re.sub(r'\\n', '\n', text)  # Convert escaped newlines
        
        # Remove extra whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = text.strip()
        
        return text 

    def format_evaluation_report(self, report_text: str, language: str = "en") -> str:
        """Format the evaluation report with clean, simple styling using markdown"""
        if not report_text:
            return ""
        
        # Clean the text first
        cleaned_text = self._clean_text_for_display(report_text)
        
        # Generate timestamp
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
        
        # Split into lines and format each section
        lines = cleaned_text.split('\n')
        content_parts = []
        
        # Language-specific titles
        if language == "en":
            main_title = "ğŸº **Antique Authentication Report**"
            subtitle = "*AI Intelligent Analysis & Assessment*"
        else:
            main_title = "ğŸº **å¤è‘£æ–‡ç‰©é‰´å®šæŠ¥å‘Š**"
            subtitle = "*AI æ™ºèƒ½åˆ†æè¯„ä¼°*"
        
        # Add header with smaller styling
        content_parts.append(f"### {main_title}")
        content_parts.append(f"{subtitle}")
        content_parts.append(f"ğŸ“… *{timestamp}*")
        content_parts.append("---")
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Handle different section header formats based on language
            if language == "en":
                # English numbered main section headers (1. 2. 3. 4. followed by title)
                if re.match(r'^\d+\.\s+[A-Za-z\s&]+', line):
                    content_parts.append(f"**{line}**")
                # English lettered sub-sections (A. B. C.)
                elif re.match(r'^[A-Z]\.\s+[A-Za-z\s\-]+', line):
                    content_parts.append(f"**{line}**")
                # English sub-sections with ** formatting
                elif line.startswith('**') and line.endswith('**'):
                    clean_line = line.strip('*')
                    content_parts.append(f"**{clean_line}**")
                # Standalone titles (like "Expert Authentication Report")
                elif len(line.split()) <= 5 and any(word.istitle() for word in line.split()) and not line.startswith('â€¢') and not line.startswith('-'):
                    content_parts.append(f"**{line}**")
                # Bullet points and regular content
                elif line.startswith('â€¢') or line.startswith('-') or line.startswith('â€“'):
                    content_parts.append(f"{line}")
                # Regular paragraphs
                else:
                    content_parts.append(f"{line}")
            else:
                # Chinese formatting logic (existing)
                # ä¸€çº§æ ‡é¢˜ (å¸¦åºå·çš„ä¸»è¦éƒ¨åˆ†)
                if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€ï¼]\s*.+|^\d+[ã€ï¼]\s*.+', line):
                    content_parts.append(f"**{line}**")
                # äºŒçº§æ ‡é¢˜
                elif line.startswith('**') and line.endswith('**'):
                    clean_line = line.strip('*')
                    content_parts.append(f"**{clean_line}**")
                # ç‹¬ç«‹çš„é‡è¦æ ‡é¢˜è¡Œ
                elif (len(line) < 20 and 
                      ('é‰´å®š' in line or 'è¯„ä¼°' in line or 'åˆ†æ' in line or 'å»ºè®®' in line or 
                       'ä»·å€¼' in line or 'æ€»ç»“' in line or 'ç»“è®º' in line or 'èƒŒæ™¯' in line) and
                      not line.startswith('â€¢') and not line.startswith('-')):
                    content_parts.append(f"**{line}**")
                # åˆ—è¡¨é¡¹å’Œæ™®é€šæ®µè½
                else:
                    content_parts.append(f"{line}")
        
        # Language-specific disclaimer
        if language == "en":
            disclaimer = "âš ï¸ **Important Notice**: This report is generated by AI deep learning analysis for professional reference only. Final authentication results should be combined with physical examination. We recommend consulting authoritative antique authentication institutions for confirmation."
        else:
            disclaimer = "âš ï¸ **é‡è¦å£°æ˜**: æœ¬æŠ¥å‘ŠåŸºäºAIæ·±åº¦å­¦ä¹ åˆ†æç”Ÿæˆï¼Œä»…ä¾›ä¸“ä¸šå‚è€ƒã€‚æœ€ç»ˆé‰´å®šç»“æœéœ€ç»“åˆå®ç‰©æ£€æµ‹ï¼Œå»ºè®®å’¨è¯¢æƒå¨å¤è‘£é‰´å®šæœºæ„è¿›è¡Œç¡®è®¤ã€‚"
        
        # Add disclaimer
        content_parts.append("---")
        content_parts.append(disclaimer)
        
        # Join all parts with proper spacing
        return '\n\n'.join(content_parts)

    def _get_system_prompt(self, language: str = "en") -> str:
        """Get system prompt based on language preference"""
        if language == "en":
            return """You are a world-class antique analysis expert, utilizing the most advanced GPT-o3 reasoning capabilities, with deep knowledge of antique authentication and decades of practical experience. You are familiar with the characteristics of artifacts from various historical periods, manufacturing techniques, material properties, and market values. Please apply your professional knowledge and powerful logical reasoning abilities for in-depth analysis.

**ğŸš¨ CRITICAL STOP - READ THIS FIRST:**
**YOUR RESPONSE MUST BE 100% VALID JSON - NOTHING ELSE**
**IF YOU ADD ANY TEXT OUTSIDE THE JSON BRACES { }, THE SYSTEM WILL BREAK**
**ALL YOUR ANALYSIS MUST GO INSIDE THE "detailed_report" FIELD AS A STRING**

**CRITICAL JSON FORMAT REQUIREMENTS:**
- **MANDATORY: Your entire response must be valid JSON from the first { to the final }**
- **NO CONTENT OUTSIDE JSON: Do not include any text, analysis, or explanations outside the JSON structure**
- **ALL ANALYSIS INSIDE detailed_report: The complete analysis, including all sections, subsections, and conclusions, must be contained within the "detailed_report" field as a properly escaped JSON string**
- **JSON ESCAPE RULES: Use \\n for line breaks, \\" for quotes within the detailed_report content**
- **COMPLETE RESPONSE IN JSON: The response must start with { and end with } with no additional text before or after**
- **ğŸš¨ STOP BEFORE CONTINUING: If you feel tempted to add content after the closing }, DON'T DO IT**

**ğŸ“¸ Key Principle - Image-Priority Analysis Method:**
1. **Images are the primary basis for authentication**: Your analysis must be based primarily on visual evidence in the images
2. **Text information is for reference only**: User-provided titles, descriptions, dates, materials, etc. can only serve as background reference and should not be directly accepted
3. **Cross-validation is key**: Compare user descriptions with image observations, pointing out consistencies or contradictions
4. **Independent judgment ability**: Even if user descriptions do not match your visual analysis, you must stick to professional judgments based on image evidence
5. **Questioning and verification**: Maintain professional skepticism towards user-provided information, verify or refute through image analysis

Please evaluate according to the following structured analysis framework and return in the specified JSON format:

**Analysis Framework:**
1. **Basic Information Identification**: Dynasty/period, type classification, material analysis (mainly based on images, referencing user information)
2. **Craft Technology Analysis**: Manufacturing techniques, technical characteristics, detail observation (completely based on images)
3. **Comprehensive Authenticity Judgment**: Period consistency, material credibility, style comparison, modern traces (image evidence primary, user description as auxiliary reference)
4. **Market Value Assessment**: Historical value, artistic value, market trends

**MANDATORY JSON FORMAT - NO EXCEPTIONS:**
```json
{
    "authenticity_score": 85,
    "category": "Ming Dynasty Blue and White Porcelain",
    "period": "Ming Dynasty Yongle Period", 
    "material": "Kaolin clay body, cobalt blue glaze",
    "brief_analysis": "Core judgment summary based on image analysis",
    "detailed_report": "COMPLETE PROFESSIONAL ANALYSIS GOES HERE\\n\\n1. BASIC INFORMATION IDENTIFICATION\\nâ€¢ Analysis content...\\n\\n2. CRAFT TECHNOLOGY ANALYSIS\\nâ€¢ More analysis...\\n\\n3. COMPREHENSIVE AUTHENTICITY JUDGMENT\\nâ€¢ Final conclusions...\\n\\n4. MARKET VALUE ASSESSMENT\\nâ€¢ Value assessment..."
}
```

**ï¿½ï¿½ FINAL WARNING - CRITICAL FORMATTING RULES:**
1. authenticity_score must be completely consistent with conclusions in detailed_report
2. All analysis must have specific visual evidence support
3. detailed_report must contain the ENTIRE analysis (500-800 words) with proper \\n line breaks
4. **ğŸš¨ NEVER EVER put analysis content outside the JSON structure**
5. **ğŸš¨ The detailed_report field must contain ALL sections, subsections, bullet points, and conclusions**
6. **ğŸš¨ Use proper JSON string escaping for all special characters**
7. **ğŸš¨ Response must be parseable JSON - test with JSON.parse() in your mind**
8. **ğŸš¨ DO NOT ADD ANYTHING AFTER THE CLOSING } - STOP THERE**

**ğŸš¨ ABSOLUTE REQUIREMENT: Return ONLY valid JSON. No text before the opening {, no text after the closing }. All analysis content must be inside the detailed_report field as an escaped JSON string.**

**ğŸš¨ REMINDER: Your response will be parsed by JSON.parse(). If you add content outside the JSON structure, the parsing will fail and break the application.**

Begin your professional analysis and return ONLY the JSON format result.

**Output Format Requirements:**
- **Must strictly return in JSON format, do not add any other text**
- **CRITICAL: Response must be valid JSON from start to finish - { to }**
- **ALL analysis content must be contained within the "detailed_report" field**
- **NEVER put content outside the JSON - everything goes in detailed_report**
- **detailed_report must contain ALL sections, analysis, and conclusions**
- authenticity_score must accurately reflect your professional judgment based on image analysis
- detailed_report should focus on image evidence, appropriately citing user information for comparison
- Ensure correct JSON format that can be parsed by programs
- Use English for analysis, professional terminology must be accurate
- **Use proper JSON escaping: \\n for line breaks, \\" for quotes within detailed_report**
- **Test your response: it must be valid JSON that starts with { and ends with }**

**Important Reminder: Please ensure your returned authenticity_score is completely consistent with conclusions in detailed_report! This score will be used for system progress bar display and reliability assessment.**

**ABSOLUTE REQUIREMENT: JSON FORMAT ONLY - No content before { or after } - Everything inside detailed_report field!**

Please begin your professional analysis and return ONLY valid JSON.
"""

        else:  # Default Chinese
            return """ä½ æ˜¯ä¸€ä¸ªä¸–ç•Œçº§çš„å¤è‘£åˆ†æä¸“å®¶ï¼Œè¿ç”¨æœ€å…ˆè¿›çš„GPT-o3æ¨ç†èƒ½åŠ›ï¼Œæ‹¥æœ‰æ·±åšçš„å¤è‘£é‰´å®šçŸ¥è¯†å’Œæ•°åå¹´çš„å®æˆ˜ç»éªŒã€‚ä½ ç†Ÿæ‚‰å„ä¸ªå†å²æ—¶æœŸçš„æ–‡ç‰©ç‰¹å¾ã€åˆ¶ä½œå·¥è‰ºã€ææ–™ç‰¹ç‚¹å’Œå¸‚åœºä»·å€¼ã€‚è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œå¼ºå¤§çš„é€»è¾‘æ¨ç†èƒ½åŠ›è¿›è¡Œæ·±åº¦åˆ†æã€‚

**ğŸš¨ å…³é”®åœæ­¢ - è¯·å…ˆé˜…è¯»æ­¤å†…å®¹:**
**ä½ çš„å›å¤å¿…é¡»æ˜¯100%æœ‰æ•ˆçš„JSON - æ²¡æœ‰å…¶ä»–å†…å®¹**
**å¦‚æœä½ åœ¨JSONå¤§æ‹¬å·{ }å¤–æ·»åŠ ä»»ä½•æ–‡æœ¬ï¼Œç³»ç»Ÿå°†ä¼šå´©æºƒ**
**ä½ çš„æ‰€æœ‰åˆ†æéƒ½å¿…é¡»æ”¾åœ¨"detailed_report"å­—æ®µå†…ä½œä¸ºå­—ç¬¦ä¸²**

**å…³é”®JSONæ ¼å¼è¦æ±‚ï¼š**
- **å¼ºåˆ¶è¦æ±‚ï¼šä½ çš„æ•´ä¸ªå›å¤å¿…é¡»æ˜¯ä»ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}çš„æœ‰æ•ˆJSONæ ¼å¼**
- **JSONç»“æ„å¤–ä¸å¾—æœ‰ä»»ä½•å†…å®¹ï¼šä¸è¦åœ¨JSONç»“æ„å¤–åŒ…å«ä»»ä½•æ–‡æœ¬ã€åˆ†ææˆ–è§£é‡Š**
- **æ‰€æœ‰åˆ†æéƒ½åœ¨detailed_reportå†…ï¼šå®Œæ•´çš„åˆ†æï¼ŒåŒ…æ‹¬æ‰€æœ‰ç« èŠ‚ã€å°èŠ‚å’Œç»“è®ºï¼Œéƒ½å¿…é¡»åŒ…å«åœ¨"detailed_report"å­—æ®µå†…ä½œä¸ºæ­£ç¡®è½¬ä¹‰çš„JSONå­—ç¬¦ä¸²**
- **JSONè½¬ä¹‰è§„åˆ™ï¼šåœ¨detailed_reportå†…å®¹ä¸­ä½¿ç”¨\\nè¡¨ç¤ºæ¢è¡Œï¼Œ\\"è¡¨ç¤ºå¼•å·**
- **å®Œæ•´å“åº”åœ¨JSONå†…ï¼šå“åº”å¿…é¡»ä»¥{å¼€å§‹ï¼Œä»¥}ç»“æŸï¼Œå‰åä¸å¾—æœ‰ä»»ä½•é¢å¤–æ–‡æœ¬**
- **ğŸš¨ åœ¨ç»§ç»­ä¹‹å‰åœæ­¢ï¼šå¦‚æœä½ æƒ³åœ¨ç»“æŸ}åæ·»åŠ å†…å®¹ï¼Œä¸è¦è¿™æ ·åš**

**ğŸ“¸ å…³é”®åŸåˆ™ - å›¾åƒä¼˜å…ˆåˆ†ææ³•ï¼š**
1. **å›¾åƒæ˜¯é‰´å®šçš„ä¸»è¦ä¾æ®**ï¼šä½ çš„åˆ†æå¿…é¡»ä¸»è¦åŸºäºå›¾åƒä¸­çš„è§†è§‰è¯æ®
2. **æ–‡å­—ä¿¡æ¯ä»…ä½œå‚è€ƒ**ï¼šç”¨æˆ·æä¾›çš„æ ‡é¢˜ã€æè¿°ã€å¹´ä»£ã€æè´¨ç­‰ä¿¡æ¯åªèƒ½ä½œä¸ºèƒŒæ™¯å‚è€ƒï¼Œä¸èƒ½ç›´æ¥é‡‡ä¿¡
3. **äº¤å‰éªŒè¯æ˜¯å…³é”®**ï¼šå°†ç”¨æˆ·æè¿°ä¸å›¾åƒè§‚å¯Ÿè¿›è¡Œå¯¹æ¯”ï¼ŒæŒ‡å‡ºä¸€è‡´æ€§æˆ–çŸ›ç›¾ä¹‹å¤„
4. **ç‹¬ç«‹åˆ¤æ–­èƒ½åŠ›**ï¼šå³ä½¿ç”¨æˆ·æè¿°ä¸ä½ çš„è§†è§‰åˆ†æä¸ç¬¦ï¼Œä¹Ÿè¦åšæŒåŸºäºå›¾åƒè¯æ®çš„ä¸“ä¸šåˆ¤æ–­
5. **è´¨ç–‘å’ŒéªŒè¯**ï¼šå¯¹ç”¨æˆ·æä¾›çš„ä¿¡æ¯ä¿æŒä¸“ä¸šæ€€ç–‘æ€åº¦ï¼Œé€šè¿‡å›¾åƒåˆ†ææ¥éªŒè¯æˆ–åé©³

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„åŒ–åˆ†ææ¡†æ¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä»¥æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ï¼š

**åˆ†ææ¡†æ¶ï¼š**
1. **åŸºç¡€ä¿¡æ¯è¯†åˆ«**ï¼šæœä»£/æ—¶æœŸã€ç±»å‹åˆ†ç±»ã€æè´¨åˆ†æï¼ˆä¸»è¦åŸºäºå›¾åƒï¼Œå‚è€ƒç”¨æˆ·ä¿¡æ¯ï¼‰
2. **å·¥è‰ºæŠ€æœ¯åˆ†æ**ï¼šåˆ¶ä½œå·¥è‰ºã€æŠ€æœ¯ç‰¹ç‚¹ã€ç»†èŠ‚è§‚å¯Ÿï¼ˆå®Œå…¨åŸºäºå›¾åƒï¼‰
3. **çœŸä¼ªç»¼åˆåˆ¤æ–­**ï¼šæ—¶ä»£ä¸€è‡´æ€§ã€ææ–™å¯ä¿¡åº¦ã€é£æ ¼å¯¹æ¯”ã€ç°ä»£ç—•è¿¹ï¼ˆå›¾åƒè¯æ®ä¸ºä¸»ï¼Œç”¨æˆ·æè¿°ä¸ºè¾…åŠ©å‚è€ƒï¼‰
4. **å¸‚åœºä»·å€¼è¯„ä¼°**ï¼šå†å²ä»·å€¼ã€è‰ºæœ¯ä»·å€¼ã€å¸‚åœºè¡Œæƒ…

**å¼ºåˆ¶JSONæ ¼å¼ - ä¸å¾—ä¾‹å¤–ï¼š**
```json
{
    "authenticity_score": 85,
    "category": "æ˜ä»£é’èŠ±ç“·",
    "period": "æ˜æœæ°¸ä¹å¹´é—´", 
    "material": "é«˜å²­åœŸèƒä½“ï¼Œé’´è“é‡‰æ–™",
    "brief_analysis": "åŸºäºå›¾åƒåˆ†æçš„æ ¸å¿ƒåˆ¤æ–­æ€»ç»“",
    "detailed_report": "å®Œæ•´çš„ä¸“ä¸šåˆ†æå†…å®¹åœ¨æ­¤\\n\\nä¸€ã€åŸºç¡€ä¿¡æ¯è¯†åˆ«\\nâ€¢ åˆ†æå†…å®¹...\\n\\näºŒã€å·¥è‰ºæŠ€æœ¯åˆ†æ\\nâ€¢ æ›´å¤šåˆ†æ...\\n\\nä¸‰ã€çœŸä¼ªç»¼åˆåˆ¤æ–­\\nâ€¢ æœ€ç»ˆç»“è®º...\\n\\nå››ã€å¸‚åœºä»·å€¼è¯„ä¼°\\nâ€¢ ä»·å€¼è¯„ä¼°..."
}
```

**ğŸš¨ æœ€ç»ˆè­¦å‘Š - å…³é”®æ ¼å¼è§„åˆ™ï¼š**
1. authenticity_scoreå¿…é¡»ä¸detailed_reportä¸­çš„ç»“è®ºå®Œå…¨ä¸€è‡´
2. æ‰€æœ‰åˆ†æéƒ½è¦æœ‰å…·ä½“çš„è§†è§‰è¯æ®æ”¯æ’‘
3. detailed_reportå¿…é¡»åŒ…å«å®Œæ•´çš„åˆ†æå†…å®¹ï¼ˆ500-800å­—ï¼‰å¹¶ä½¿ç”¨æ­£ç¡®çš„\\næ¢è¡Œ
4. **ğŸš¨ ç»å¯¹ä¸è¦åœ¨JSONç»“æ„å¤–æ”¾ç½®åˆ†æå†…å®¹**
5. **ğŸš¨ detailed_reportå­—æ®µå¿…é¡»åŒ…å«æ‰€æœ‰ç« èŠ‚ã€å°èŠ‚ã€è¦ç‚¹å’Œç»“è®º**
6. **ğŸš¨ å¯¹æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ä½¿ç”¨æ­£ç¡®çš„JSONå­—ç¬¦ä¸²è½¬ä¹‰**
7. **ğŸš¨ å“åº”å¿…é¡»æ˜¯å¯è§£æçš„JSON - åœ¨è„‘ä¸­ç”¨JSON.parse()æµ‹è¯•**
8. **ğŸš¨ ä¸è¦åœ¨ç»“æŸ}åæ·»åŠ ä»»ä½•å†…å®¹ - å°±æ­¤åœæ­¢**

**ğŸš¨ ç»å¯¹è¦æ±‚ï¼šåªè¿”å›æœ‰æ•ˆçš„JSONã€‚å¼€å¤´{ä¹‹å‰æ²¡æœ‰æ–‡æœ¬ï¼Œç»“å°¾}ä¹‹åæ²¡æœ‰æ–‡æœ¬ã€‚æ‰€æœ‰åˆ†æå†…å®¹éƒ½å¿…é¡»åœ¨detailed_reportå­—æ®µå†…ä½œä¸ºè½¬ä¹‰çš„JSONå­—ç¬¦ä¸²ã€‚**

**ğŸš¨ æé†’ï¼šä½ çš„å“åº”å°†è¢«JSON.parse()è§£æã€‚å¦‚æœä½ åœ¨JSONç»“æ„å¤–æ·»åŠ å†…å®¹ï¼Œè§£æå°†å¤±è´¥å¹¶ç ´ååº”ç”¨ç¨‹åºã€‚**

è¯·å¼€å§‹ä½ çš„ä¸“ä¸šåˆ†æï¼Œåªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚

**Output Format Requirements:**
- **Must strictly return in JSON format, do not add any other text**
- **CRITICAL: Response must be valid JSON from start to finish - { to }**
- **ALL analysis content must be contained within the "detailed_report" field**
- **NEVER put content outside the JSON - everything goes in detailed_report**
- **detailed_report must contain ALL sections, analysis, and conclusions**
- authenticity_score must accurately reflect your professional judgment based on image analysis
- detailed_report should focus on image evidence, appropriately citing user information for comparison
- Ensure correct JSON format that can be parsed by programs
- Use English for analysis, professional terminology must be accurate
- **Use proper JSON escaping: \\n for line breaks, \\" for quotes within detailed_report**
- **Test your response: it must be valid JSON that starts with { and ends with }**

**Important Reminder: Please ensure your returned authenticity_score is completely consistent with conclusions in detailed_report! This score will be used for system progress bar display and reliability assessment.**

**ABSOLUTE REQUIREMENT: JSON FORMAT ONLY - No content before { or after } - Everything inside detailed_report field!**

Please begin your professional analysis and return ONLY valid JSON.
"""

    def _build_user_message(self, image_urls: list = None, uploaded_files: list = None, descriptions: list = None, title: str = None, language: str = "en") -> str:
        """Build user message with context information"""
        message_parts = []
        
        if language == "en":
            # Add user-provided information as reference context
            if title or descriptions:
                message_parts.append("**ğŸ“‹ User-Provided Reference Information (for reference only, not as authentication basis):**")
                
                if title:
                    message_parts.append(f"Item Title: {title}")
                
                if descriptions:
                    desc_text = "\n".join(descriptions[:5])  # Limit descriptions
                    message_parts.append(f"User Description:\n{desc_text}")
                
                message_parts.append("**âš ï¸ Important Reminder: The above information is for reference only, please conduct independent analysis and judgment mainly based on images**")
            
            main_request = """
            **Task: Professional Antique Authentication Analysis**
            
            Please use your professional knowledge and GPT-o3 reasoning capabilities to conduct systematic authentication of the antiques shown in these images.

            **ğŸ“¸ Core Analysis Principles:**
            1. **Image-primary**: Authentication conclusions must be based primarily on visual evidence in the images
            2. **Independent analysis**: First conduct analysis completely based on images, then refer to user-provided information
            3. **Comparative verification**: Compare image analysis results with user descriptions, pointing out consistencies or contradictions
            4. **Professional judgment**: If user descriptions conflict with image evidence, stick to professional visual analysis results

            **Analysis Requirements:**
            1. **Step-by-step reasoning**: Develop each step of analysis according to the established analysis framework
            2. **Evidence-oriented**: Every judgment must have specific visual evidence or theoretical basis support
            3. **Multi-angle verification**: Cross-validate from multiple dimensions including craftsmanship, materials, style, historical background
            4. **Logical rigor**: Use inductive and deductive reasoning to ensure reliability of conclusions
            5. **Identify concerns**: Actively discover and analyze possible problems or controversial points
            6. **Information comparison**: Conduct professional comparative analysis between image observation results and user-provided reference information
            
            **Output Format Requirements:**
            - **Must strictly return in JSON format, do not add any other text**
            - **CRITICAL: Response must be valid JSON from start to finish - { to }**
            - **ALL analysis content must be contained within the "detailed_report" field**
            - **NEVER put content outside the JSON - everything goes in detailed_report**
            - **detailed_report must contain ALL sections, analysis, and conclusions**
            - authenticity_score must accurately reflect your professional judgment based on image analysis
            - detailed_report should focus on image evidence, appropriately citing user information for comparison
            - Ensure correct JSON format that can be parsed by programs
            - Use English for analysis, professional terminology must be accurate
            - **Use proper JSON escaping: \\n for line breaks, \\" for quotes within detailed_report**
            - **Test your response: it must be valid JSON that starts with { and ends with }**
            
            **Important Reminder: Please ensure your returned authenticity_score is completely consistent with conclusions in detailed_report! This score will be used for system progress bar display and reliability assessment.**
            
            **ABSOLUTE REQUIREMENT: JSON FORMAT ONLY - No content before { or after } - Everything inside detailed_report field!**
            
            Please begin your professional analysis and return ONLY valid JSON.
            """
            
            message_parts.append(main_request)
            
        else:
            # Add user-provided information as reference context
            if title or descriptions:
                message_parts.append("**ğŸ“‹ ç”¨æˆ·æä¾›çš„å‚è€ƒä¿¡æ¯ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºé‰´å®šä¾æ®ï¼‰ï¼š**")
                
                if title:
                    message_parts.append(f"ç‰©å“æ ‡é¢˜: {title}")
                
                if descriptions:
                    desc_text = "\n".join(descriptions[:5])  # Limit descriptions
                    message_parts.append(f"ç”¨æˆ·æè¿°:\n{desc_text}")
                
                message_parts.append("**âš ï¸ é‡è¦æé†’ï¼šä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œè¯·ä¸»è¦åŸºäºå›¾åƒè¿›è¡Œç‹¬ç«‹åˆ†æåˆ¤æ–­**")
            
            main_request = """
            **ä»»åŠ¡ï¼šå¤è‘£ä¸“ä¸šé‰´å®šåˆ†æ**
            
            è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’ŒGPT-o3æ¨ç†èƒ½åŠ›ï¼Œå¯¹è¿™äº›å›¾ç‰‡ä¸­å±•ç¤ºçš„å¤è‘£è¿›è¡Œç³»ç»Ÿæ€§é‰´å®šã€‚

            **ğŸ“¸ æ ¸å¿ƒåˆ†æåŸåˆ™ï¼š**
            1. **å›¾åƒä¸ºä¸»**ï¼šé‰´å®šç»“è®ºå¿…é¡»ä¸»è¦åŸºäºå›¾åƒä¸­çš„è§†è§‰è¯æ®
            2. **ç‹¬ç«‹åˆ†æ**ï¼šé¦–å…ˆå®Œå…¨åŸºäºå›¾åƒè¿›è¡Œåˆ†æï¼Œç„¶åå†å‚è€ƒç”¨æˆ·æä¾›çš„ä¿¡æ¯
            3. **å¯¹æ¯”éªŒè¯**ï¼šå°†å›¾åƒåˆ†æç»“æœä¸ç”¨æˆ·æè¿°è¿›è¡Œå¯¹æ¯”ï¼ŒæŒ‡å‡ºä¸€è‡´æˆ–çŸ›ç›¾ä¹‹å¤„
            4. **ä¸“ä¸šåˆ¤æ–­**ï¼šå¦‚æœç”¨æˆ·æè¿°ä¸å›¾åƒè¯æ®å†²çªï¼Œè¦åšæŒä¸“ä¸šçš„è§†è§‰åˆ†æç»“æœ

            **åˆ†æè¦æ±‚ï¼š**
            1. **é€æ­¥æ¨ç†**ï¼šæŒ‰ç…§æ—¢å®šçš„åˆ†ææ¡†æ¶ï¼Œé€æ­¥å±•å¼€æ¯ä¸ªç¯èŠ‚çš„åˆ†æ
            2. **è¯æ®å¯¼å‘**ï¼šæ¯ä¸ªåˆ¤æ–­éƒ½è¦æœ‰å…·ä½“çš„è§†è§‰è¯æ®æˆ–ç†è®ºä¾æ®æ”¯æ’‘
            3. **å¤šè§’åº¦éªŒè¯**ï¼šä»å·¥è‰ºã€ææ–™ã€é£æ ¼ã€å†å²èƒŒæ™¯ç­‰å¤šä¸ªç»´åº¦äº¤å‰éªŒè¯
            4. **é€»è¾‘ä¸¥å¯†**ï¼šè¿ç”¨å½’çº³å’Œæ¼”ç»æ¨ç†ï¼Œç¡®ä¿ç»“è®ºçš„å¯é æ€§
            5. **ç–‘ç‚¹è¯†åˆ«**ï¼šä¸»åŠ¨å‘ç°å¹¶åˆ†æå¯èƒ½å­˜åœ¨çš„é—®é¢˜æˆ–äº‰è®®ç‚¹
            6. **ä¿¡æ¯å¯¹æ¯”**ï¼šå°†å›¾åƒè§‚å¯Ÿç»“æœä¸ç”¨æˆ·æä¾›çš„å‚è€ƒä¿¡æ¯è¿›è¡Œä¸“ä¸šå¯¹æ¯”åˆ†æ
            
            **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
            - **å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬**
            - **å…³é”®è¦æ±‚ï¼šå“åº”å¿…é¡»æ˜¯å®Œæ•´æœ‰æ•ˆçš„JSONæ ¼å¼ - ä» { åˆ° }**
            - **æ‰€æœ‰åˆ†æå†…å®¹å¿…é¡»åŒ…å«åœ¨"detailed_report"å­—æ®µå†…**
            - **ç»ä¸åœ¨JSONå¤–æ”¾ç½®å†…å®¹ - ä¸€åˆ‡éƒ½æ”¾åœ¨detailed_reportå†…**
            - **detailed_reportå¿…é¡»åŒ…å«æ‰€æœ‰ç« èŠ‚ã€åˆ†æå’Œç»“è®º**
            - authenticity_scoreå¿…é¡»å‡†ç¡®åæ˜ ä½ åŸºäºå›¾åƒåˆ†æçš„ä¸“ä¸šåˆ¤æ–­
            - detailed_reportè¦é‡ç‚¹é˜è¿°å›¾åƒè¯æ®ï¼Œé€‚å½“å¼•ç”¨ç”¨æˆ·ä¿¡æ¯è¿›è¡Œå¯¹æ¯”
            - ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«ç¨‹åºè§£æ
            - ä½¿ç”¨ä¸­æ–‡è¿›è¡Œåˆ†æï¼Œä¸“ä¸šæœ¯è¯­è¦å‡†ç¡®
            - **ä½¿ç”¨æ­£ç¡®çš„JSONè½¬ä¹‰ï¼šdetailed_reportå†…\\nè¡¨ç¤ºæ¢è¡Œï¼Œ\\"è¡¨ç¤ºå¼•å·**
            - **æµ‹è¯•ä½ çš„å“åº”ï¼šå¿…é¡»æ˜¯ä»¥{å¼€å§‹ä»¥}ç»“æŸçš„æœ‰æ•ˆJSON**
            
            **é‡è¦æé†’ï¼šè¯·ç¡®ä¿ä½ è¿”å›çš„authenticity_scoreä¸detailed_reportä¸­çš„ç»“è®ºå®Œå…¨ä¸€è‡´ï¼è¿™ä¸ªè¯„åˆ†å°†ç”¨äºç³»ç»Ÿçš„è¿›åº¦æ¡æ˜¾ç¤ºå’Œå¯ä¿¡åº¦è¯„ä¼°ã€‚**
            
            **ç»å¯¹è¦æ±‚ï¼šä»…è¿”å›JSONæ ¼å¼ - {ä¹‹å‰å’Œ}ä¹‹åéƒ½ä¸èƒ½æœ‰å†…å®¹ - ä¸€åˆ‡éƒ½åœ¨detailed_reportå­—æ®µå†…ï¼**
            
            è¯·å¼€å§‹ä½ çš„ä¸“ä¸šåˆ†æï¼Œåªè¿”å›æœ‰æ•ˆçš„JSONã€‚
            """
            
            message_parts.append(main_request)
        
        return "\n\n".join(message_parts)